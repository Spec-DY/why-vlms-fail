# VLM Rule Following Test

A comprehensive testing framework for evaluating Vision Language Models (VLMs) on spatial and temporal reasoning abilities using chess scenarios.

## ğŸ“‹ Overview

This project systematically tests VLMs through generated chess board images to evaluate their capabilities in:

- **Spatial Reasoning**: Board position recognition, relative directions, diagonal relationships, etc.
- **Rule Understanding**: Chess rules application (piece movements, en passant, castling, etc.)
- **Temporal Reasoning**: Tracking piece movement sequences and understanding state changes

Each test includes a **verification mechanism** to ensure the model can correctly recognize the board before testing its reasoning abilities.

## âœ¨ Key Features

### ğŸ¯ Four Test Suites

**1. Spatial Test 0 - Pure Spatial Reasoning**

- Tests fundamental spatial understanding (no chess knowledge required)
- Includes: same file/rank detection, diagonal recognition, relative directions, path clearance
- ~90 test cases

**2. Spatial Test 1 - Rule Following Baseline**

- Tests movement rules for all 6 piece types (King, Queen, Rook, Bishop, Knight, Pawn)
- Includes: legal moves, blocked paths, castling rules (through check & in check)
- 100+ test cases

**3. Temporal Test 0 - Pure Temporal Reasoning**

- Tests sequence understanding and state tracking
- Includes: movement tracking, state comparison, sequence comprehension
- Multi-frame image tests

**4. Temporal Test 1 - Temporal Rule Following**

- Tests complex time-dependent chess rules
- Includes: En Passant, Castling with temporal constraints
- Event recognition and rule application

### ğŸ” Verification Mechanism

Every test case includes two-step verification:

1. **Board Recognition Verification**: Ensures the model correctly sees the board and pieces
2. **Ability Testing**: Only counts toward results after passing verification

This design distinguishes between "cannot see the board" and "insufficient reasoning ability" failure modes.

### ğŸ”Œ Multi-Model Support

Supports any OpenAI-compatible API:

- **DashScope** (Alibaba)
- **Novita AI**
- **XAI** (Grok)
- **Custom endpoints** (easily extensible)
- **Dummy Model** (for testing the framework)

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8+
pip install -r requirements.txt
```

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd rulefollow_test

# Install dependencies
pip install -r requirements.txt

# Or install in development mode
pip install -e .
```

### Configuration

Create a `.env` file in the project root:

```env
# For DashScope (Alibaba Qwen models)
DASHSCOPE_API_KEY=your_api_key_here
DASHSCOPE_MODEL=qwen-vl-max
DASHSCOPE_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1

# For Novita AI
NOVITA_API_KEY=your_api_key_here
NOVITA_BASE_URL=https://api.novita.ai/openai
NOVITA_MODEL=qwen/qwen3-vl-235b-a22b-thinking

# For XAI (Grok)
XAI_API_KEY=your_api_key_here
XAI_MODEL=grok-4-fast-reasoning
XAI_BASE_URL=https://api.x.ai/v1
```

### Running Tests

```bash
# Run Spatial Test 0 (Pure Spatial Reasoning)
python examples/run_spatial_test_0.py

# Run Spatial Test 1 (Rule Following)
python examples/run_spatial_test_1.py

# Run Temporal Test 0 (Pure Temporal Reasoning)
python examples/run_temporal_test_0.py

# Run Temporal Test 1 (Temporal Rule Following)
python examples/run_temporal_test_1.py
```

### Customizing Test Parameters

Edit the configuration section in each run file:

```python
# In examples/run_spatial_test_0.py
N_CASES_PER_TYPE = 18      # Number of cases per test type
SEED = 57                  # Random seed for reproducibility
MODEL_TYPE = "xai"         # Options: "dummy", "dashscope", "novita", "xai"
RATE_LIMIT_REQUESTS = 0    # Number of requests before pausing
RATE_LIMIT_PAUSE = 0       # Pause duration in seconds
```

## ğŸ“‚ Project Structure

```
rulefollow_test/
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ pieces/          # Chess piece PNG images (wk.png, bq.png, etc.)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ spatial/         # Spatial reasoning tests
â”‚   â”‚   â”œâ”€â”€ test_0_pure_ability.py
â”‚   â”‚   â”œâ”€â”€ test_0_generator.py
â”‚   â”‚   â”œâ”€â”€ test_1_rule_following.py
â”‚   â”‚   â”œâ”€â”€ test_1_generator.py
â”‚   â”‚   â”œâ”€â”€ spatial_test_base.py
â”‚   â”‚   â””â”€â”€ verification_generator.py
â”‚   â”œâ”€â”€ temporal/        # Temporal reasoning tests
â”‚   â”‚   â”œâ”€â”€ test_0_pure_ability.py
â”‚   â”‚   â”œâ”€â”€ test_0_generator.py
â”‚   â”‚   â”œâ”€â”€ test_1_rule_following.py
â”‚   â”‚   â””â”€â”€ test_1_generator.py
â”‚   â”œâ”€â”€ board_generator.py      # Chess board image generation
â”‚   â”œâ”€â”€ model_client.py          # Model API clients
â”‚   â””â”€â”€ data_structures.py       # Data models and utilities
â”œâ”€â”€ examples/            # Example run scripts
â”‚   â”œâ”€â”€ run_spatial_test_0.py
â”‚   â”œâ”€â”€ run_spatial_test_1.py
â”‚   â”œâ”€â”€ run_temporal_test_0.py
â”‚   â””â”€â”€ run_temporal_test_1.py
â”œâ”€â”€ output/             # Test results (auto-generated)
â””â”€â”€ requirements.txt
```

## ğŸ§ª Test Output

Each test run generates:

### Generated Files

- **PNG Images**: Visual test cases with highlighted squares and pieces
- **JSON Results**: Detailed results with verification and accuracy metrics

### Output Structure

```
output/
â””â”€â”€ spatial_test_0_MMDD_HHMMSS/
    â”œâ”€â”€ *.png                    # Test case images
    â””â”€â”€ test_0_results.json      # Results with summary
```

### Results JSON Format

```json
{
  "summary": {
    "model_name": "grok-4-fast-reasoning",
    "total_cases": 90,
    "board_recognition": {
      "verified_correctly": 85,
      "failed_to_recognize": 5,
      "verification_rate": 0.944
    },
    "test_accuracy": {
      "correct_among_verified": 78,
      "total_verified": 85,
      "accuracy_given_verified": 0.918,
      "overall_accuracy": 0.867
    },
    "accuracy_by_type_verified_only": {
      "diagonal": {"correct": 15, "total": 18, "accuracy": 0.833},
      "same_line_same_file": {"correct": 9, "total": 10, "accuracy": 0.900}
    }
  },
  "detailed_results": [...]
}
```

## ğŸ’¡ How It Works

### 1. Test Case Generation

Each test automatically generates diverse cases:

```python
# Spatial Test 0 Generator
generator = SpatialTest0Generator(seed=42)
cases = generator.generate_all(n_per_type=10)
# Generates: same file, same rank, diagonal, direction, path clear tests
```

### 2. Board Image Creation

Uses custom chess piece images to generate visual test cases:

```python
board_gen = ChessBoardGenerator()
img = board_gen.create_board_with_pieces(
    pieces={"e4": "N", "f6": "n"},  # Uppercase=white, lowercase=black
    highlighted_squares=["e4", "f6"]
)
```

### 3. Combined Verification + Testing

Each query includes both verification and test questions:

```
Verification: What square is the piece on?
Main Question: Can this knight move to the highlighted square?
```

### 4. Result Analysis

The framework tracks:

- **Verification Rate**: How many boards were correctly recognized
- **Test Accuracy (Verified)**: Accuracy among correctly recognized boards
- **Overall Accuracy**: Total accuracy including verification failures
- **Type Breakdown**: Performance by test type and subtype

## ğŸ”§ Adding New Models

Extend the `OpenAICompatibleModelClient` class:

```python
class YourModelClient(OpenAICompatibleModelClient):
    DEFAULT_BASE_URL = "https://api.yourservice.com/v1"
    ENV_API_KEY = "YOUR_API_KEY"
    ENV_BASE_URL = "YOUR_BASE_URL"
    ENV_MODEL = "YOUR_MODEL"
    SERVICE_NAME = "YourService"
```

Add to `.env`:

```env
YOUR_API_KEY=your_key
YOUR_MODEL=your_model_name
```

## ğŸ› ï¸ Advanced Usage

### Custom Test Cases

```python
# Create custom test
test = SpatialTest0(
    base_output_dir="./custom_output",
    n_cases_per_type=20,  # More cases per type
    seed=123,             # Different random seed
    auto_timestamp=True,
    rate_limit_requests=50,  # Pause every 50 requests
    rate_limit_pause=60      # 60 second pause
)

# Generate and run
test.generate_test_cases()
test.create_test_images()
results, stats = test.run_test(model_client)
```

### Rate Limiting

For API rate limits, configure pause intervals:

```python
test = SpatialTest1(
    rate_limit_requests=50,  # Pause after 50 requests
    rate_limit_pause=60      # Wait 60 seconds
)
```

### Batch Testing Multiple Models

```python
models = ["dashscope", "novita", "xai"]
for model_type in models:
    model_client = create_client(model_type)
    results, stats = test.run_test(model_client)
    # Results auto-saved to separate directories
```

## ğŸ”¬ Test Types Explained

### Spatial Test 0 Categories

- **Same Line**: File (vertical) and Rank (horizontal) alignment
- **Diagonal**: Diagonal relationship detection
- **Relative Position**: 8 directional relationships (N, NE, E, SE, S, SW, W, NW)
- **Path Clear**: Obstacle detection between squares

### Spatial Test 1 Categories

Each piece type has 3 subtests:

- **Clear Path**: Valid moves without obstacles
- **Blocked Path**: Invalid moves due to blocking pieces
- **Invalid Move**: Moves that violate piece movement rules

Plus special castling tests:

- **Castling Through Check**: Cannot castle through attacked squares
- **Castling In Check**: Cannot castle while in check

### Temporal Test Categories

- **Movement Tracking**: Identify which piece moved
- **State Comparison**: Compare board states across frames
- **Sequence Understanding**: Understand move sequences
- **Rule Application**: Apply time-dependent rules (en passant timing)

# Condition Ladder

Here is a simple usage guide in English for running the temporal levels. You can save this as `USAGE.md`.

---

## How to use the `run/run_temporal_levels.py`

### 1\. Run Specific Levels

If you only want to test specific levels (e.g., Level 1 and Level 4):

```bash
python run/run_temporal_levels.py -l 1 4 --model xai
```

_(Note: Use `-l` followed by the level numbers separated by spaces)_

### 2\. Run All Levels (Recommended)

To run all available tests from Level 1 to Level 4 in one go:

```bash
python run/run_temporal_levels.py --all --model xai
```

---

## âš™ï¸ Common Arguments

| Argument                | Short | Type         | Default      | Description                                                                                              |
| :---------------------- | :---- | :----------- | :----------- | :------------------------------------------------------------------------------------------------------- |
| **`--levels`**          | `-l`  | `int` (list) | `None`       | Specific level numbers to run (e.g., `-l 1 2 3`). Mutually exclusive with `--all`.                       |
| **`--all`**             |       | `flag`       | `False`      | Run all available levels (currently 1-4). Mutually exclusive with `--levels`.                            |
| **`--model`**           | `-m`  | `str`        | `"dummy"`    | Model client to use. Options: `dummy`, `novita`, `dashscope`, `xai`.                                     |
| **`--n-cases`**         | `-n`  | `int`        | `None`       | Number of test cases to generate per level. If not set, uses the level's default (usually 60-100).       |
| **`--seed`**            | `-s`  | `int`        | `42`         | Random seed for reproducibility of test case generation.                                                 |
| **`--output`**          | `-o`  | `str`        | `"./output"` | Base directory for saving output results.                                                                |
| **`--dummy-pass-rate`** |       | `float`      | `0.8`        | **Only for `dummy` model.** Probability (0.0-1.0) that the dummy model passes the verification question. |
| **`--rate-limit`**      |       | `int`        | `0`          | Number of requests to process before pausing. `0` means no limit.                                        |
| **`--rate-pause`**      |       | `int`        | `0`          | Duration in seconds to pause when the rate limit is reached.                                             |

---

## ğŸ“‚ Where are the results?

After the script finishes, check the `output/` directory:

1.  **Summary Report (Best for overview)**:

    - File name: `output/temporal_levels_summary_YYYYMMDD_...json`
    - Contains accuracy statistics and comparisons for all run levels.

2.  **Detailed Results**:

    - Each level creates its own folder, e.g., `output/temporal_level_1_.../`
    - Contains individual case images and detailed JSON logs.
